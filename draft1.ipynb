{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The second course in the Columbia University MicroMaster's series is Machine Learning. I noticed a few posts in which people wanted more details on the derivation of least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep the notation consistent with what is shown in the lecture slides in order to avoid any confusion.\n",
    "\n",
    "Let $x_i \\in \\mathbb{R}^{d+1}, y_i \\in \\mathbb{R}$, and $w \\in \\mathbb{R}^{d+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write each vector as column vectors as follows:\n",
    "$$\n",
    "x_i =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "x_{i1} \\\\\n",
    "x_{i2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{id} \\\\\n",
    "\\end{pmatrix}\n",
    ", w =\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_d\n",
    "\\end{pmatrix}\n",
    ".\n",
    "$$\n",
    "\n",
    "Note that the $1$ is included in the vector for $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla_w \\mathcal{L} = 0 \\implies \\sum_{i=1}^n \\nabla_w (y_i^2 \\color{red}{- 2w^Tx_iy_i} + \\color{blue}{w^Tx_ix_i^Tw)} = 0,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and we would like to show that solving this yields\n",
    "\n",
    "$$\n",
    "w_{LS} = \\left(\\sum_{i=1}^n x_ix_i^T \\right)^{-1} \\left( \\sum_{i=1}^n y_ix_i \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "Suppose $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is a function which takes the input vector $x \\in \\mathbb{R}^n$ and produces as output $\\mathbf{f}(x) \\in \\mathbb{R}^m$. Then the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) matrix is defined as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Note that if $m=1$, then $\\mathbf{f}$ is a scalar and the Jacobian is reduced to a $1 \\times n$ matrix; this row vector of partial derivates is the [gradient](https://en.wikipedia.org/wiki/Gradient) of $\\mathbf{f}$,\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial x_{1}} & \\cdots & \\frac{\\partial f}{\\partial x_n} \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Let's keep this in our back pocket, as we'll be using this fact shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First term $y_i^2$\n",
    "The gradient of the first term is simply zero, because it does not depend on $w$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle term $\\color{red}{-2w^Tx_iy_i}$\n",
    "\n",
    "We can write this out as a dot product between $w^T$ and $x_i$ (remember that $y_i$ is a scalar, and the dot product is a scalar, so swapping them makes no difference).\n",
    "\n",
    "$$\n",
    "-2\n",
    "\\begin{pmatrix}\n",
    "w_0 & w_1 & w_2 & \\cdots & w_d\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "x_{i1} \\\\\n",
    "x_{i2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{id} \\\\\n",
    "\\end{pmatrix}\n",
    "y_i\n",
    "= \n",
    "-2y_i\\left(\n",
    "w_0 + w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_d x_{id}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Note that this result is a scalar, so taking the transpose of $w^T\\cdot x_i$ doesn't change anything. We now take the gradient with respect to $w$, which gives us a $d+1$ dimensional vector,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\quad -2y_i \\nabla_w w^Tx_i \\\\\n",
    "&= -2y_i \\nabla_w (w^T x_i)^T \\\\\n",
    "&= -2y_i \\nabla_w x_i^T w \\\\\n",
    "&= -2y_i x_i^T,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we have used our definition as follows:\n",
    "\n",
    "$$\n",
    "-2y_i\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial}{\\partial w_0}\n",
    "\\left(\n",
    "w_0 + w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_d x_{id}\n",
    "\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial w_d}\n",
    "\\left(\n",
    "w_0 + w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_d x_{id}\n",
    "\\right) \\\\\n",
    "\\end{pmatrix}^T\n",
    "=\n",
    "-2y_i\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "x_{id} \\\\\n",
    "\\end{pmatrix}^T\n",
    "=\n",
    "-2y_i x_i^T.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last term $\\color{blue}{w^Tx_ix_i^Tw}$\n",
    "\n",
    "Since the first two terms turned out to be scalars (prior to taking the gradient), we would expect the same of this term. Let us examine the dimensions to be sure,\n",
    "\n",
    "$$\n",
    "\\underbrace{\\; w^T \\;}_{1 \\times (d+1)}\n",
    "\\underbrace{\\; x_i\\;}_{(d+1) \\times 1}\n",
    "\\underbrace{\\;x_i^T\\;}_{1 \\times (d+1)}\n",
    "\\underbrace{\\;w\\;}_{(d+1) \\times 1}\n",
    "$$\n",
    "\n",
    "In fact, this is expression is in quadratic form. To see how that helps us, it may be instructive to consider a simple example first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
